Description- This project was mainly used to sort terabytes of data using Shared Memory, Hadoop, Spark and MPI on Amamzon AWS using single node and 8 node cluster.

Installation and Execution
First install all the necessary things like gcc,ant version,exporting gensort etc
For every configuration first follow the basic scripts in the script folder and install common softwares.

For further execution follow the respective scripts or the steps provided below.For example for Hadoop Single node follow Hadoop single node script for installation in script folder

1.Shared Memory Benchmarking
Execute below steps to execute Shared_Memory_Sort program

1) Compile code:-
	gcc -o divide Shared_Memory_Terasort.c -lm
	gcc -o sort Shared_Memory_Terasort.c -lm
	gcc -o merge Shared_Memory_Terasort.c -lm

2) Execute shared_memory_sort program.
	./divide filename

5) After completion we will get the sorted file with below name sorted.txt

2.Hadoop Benchmarking

sudo apt-get install ssh
sudo apt-get install rsync

wget http://www-us.apache.org/dist/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gz

To setup single node of Hadoop we need to do following configuration changes written below in /etc/hadoop/ file:

core-site.xml
<property>
  <name>fs.default.name</name>
  <value>hdfs://ec2-13-59-246-215.us-east-2.compute.amazonaws.com:54310</value>
  <description>The name of the default file system.  A URI whose
  scheme and authority determine the FileSystem implementation.  The
  uri's scheme determines the config property (fs.SCHEME.impl) naming
  the FileSystem implementation class.  The uri's authority is used to
  determine the host, port, etc. for a filesystem.</description>
</property>

hdfs-site.xml

<property>
  <name>dfs.replication</name>
  <value>1</value>
  <description>Default block replication.
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
  </description>
</property>

<property>
  <name>dfs.namenode.name.dir</name>
  <value>/mnt/raid/tmp/namenode</value>
</property>

<property>
  <name>dfs.blocksize</name>
  <value>268435456</value>
</property>

<property>
  <name>dfs.namenode.handler.count</name>
  <value>100</value>
</property>

<property>
  <name>dfs.datanode.data.dir</name>
  <value>/mnt/raid/tmp/datanode</value>
</property>

yarn-site.xml

<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
<property>
<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value></property>
<property>
<name>yarn.resourcemanager.resource-tracker.address</name>
<value>ec2-13-59-246-215.us-east-2.compute.amazonaws.com:9025</value>
</property>
<property>
<name>yarn.resourcemanager.scheduler.address</name>
<value>ec2-13-59-246-215.us-east-2.compute.amazonaws.com:9030</value>
</property>
<property>
<name>yarn.resourcemanager.address</name>
<value>ec2-13-59-246-215.us-east-2.compute.amazonaws.com:9050</value>
</property>
<property>
<name>yarn.resourcemanager.webapp.address</name>
<value>ec2-13-59-246-215.us-east-2.compute.amazonaws.com:9006</value>
</property>
<property>
<name>yarn.resourcemanager.admin.address</name>
<value>ec2-13-59-246-215.us-east-2.compute.amazonaws.com:9008</value>
</property><!---->
<property>
<name>yarn.nodemanager.vmem-pmem-ratio</name>
<value>2.1</value>
</property>


/conf/slave 
Change this file default local host to aws instance name. After doing the above configuration changes we need to format hdfs

mapred-site.xml

<property>
  <name>mapred.job.tracker</name>
  <value>ec2-13-59-246-215.us-east-2.compute.amazonaws.com:54311</value>
  <description>The host and port that the MapReduce job tracker runs
  at.  If "local", then jobs are run in-process as a single map
  and reduce task.
  </description>
</property>

vi .bashrc 

export CONF=/home/ubuntu/hadoop-2.7.4/etc/hadoop
export JAVA_HOME=/usr/lib/jvm/java-8-oracle
export PATH=$PATH:$/home/ubuntu/hadoop-2.7.4/bin

Configure the Raid using Raid script provided in script folder

Running Hadoop
 
./bin/hdfs namenode –format 
then we need to start all services ./sbin/start-all.sh through the start all scripts.

*Create directory in HDFS*
./bin/hadoop dfs –mkdir /mnt/raid/TeraSort/HDFSIn

*Copy the file generated by (Gensort) from local directory to a directory in HDFS*
./bin/hadoop dfs -copyFromLocal /mnt/raid/TeraSort/Input/actualInput.txt  /mnt/raid/TeraSort/HDFSIn/output

Compilation and run
Copy the hadoop code from your system to  /mnt/raid/TeraSort in aws
/usr/local/Hadoop/Hadoop2.7.4/bin/hadoop com.sun.tools.javac.Main TeraSort.java
jar cf TeraSort.jar TeraSort*.class
/usr/local/Hadoop/Hadoop2.7.4/bin/hadoop jar TerSort.jar TeraSort /mnt/raid/TeraSort/In /mnt/raid/output

For single node i3large cluster, we have run an experiment for approx 128 GB dataset, and for single node i34xlarge cluster, we have run an experiment for approx 1 TB dataset.

For an 8 node i3large cluster, we have run an assignment for approx 1 TB dataset.

To setup 8 node cluster
To set multiple nodes for hadoop we need to do keep the above configuration with few extra changes on below
mapred-site.xml
<property>
<name>mapreduce.cluster.local.dir</name>
<value>/mnt/raid/TeraSort</value>
</property>
<property>
<name>mapreduce.jobtracker.system.dir</name>			
<value>/mnt/raid/TeraSort</value>
</property>
<property>
<name>mapreduce.jobtracker.staging.root.dir</name>		
<value>/mnt/raid/TeraSort</value>
</property>
<property>
<name>mapreduce.cluster.temp.dir</name>		
<value>/mnt/raid/TeraSort</value>
</property>
<property>
<name>mapred.child.java.opts</name>
<value>-Xmx2048m-XX:+UseParallelOldGC</value>
</property>

hadoop-env.sh
export	HADOOP_CLIENT_OPTS="-Xmx2048m $HADOOP_CLIENT_OPTS"
export	HADOOP_PORTMAP_OPTS="-Xmx2048m -XX:+UseParallelOldGC	
$HADOOP_PORTMAP_OPTS"

follow the compilation and run steps for single node cluster after making changes

3.Spark Benchmarking

Running Experiment on Single and Multiple nodes
Need to export AWSAccessKeyId and AWSSecretKey then we need to run the below 
~/home/sandeep/Desktop/CS553/spark-1.6.3-bin-hadoop-2.7.4/ec2/spark-ec2 --keypair=Ec2s --identity-file=/home/sandeep/Desktop/EC2/PA2sandeepabhinav.pem –region=us-east-2 –zone=us-east-2a –instance-type=c3.large –slaves=7 –ebs-vol-size=1024 –spot-price=0.03 launch spark
We need to do additional configuration as running sort for 128 GB and 1 TB as EBS volumes are ephemeral and hence we need to make these disks permanent.
Changes that must be done to ensure persistent hdfs:
1. ./ephemeral-hdfs/bin/stop-all.sh
2. sed -I s#vol/persistent-hdfs#vol0/persistent-hdfs#g’~/persistent-hdfs/conf/core-site.xml
3. ~/spark-ec2/copy-dir.sh~/persistent-hdfs/conf/core-site.xml
4. ~/persistent-hdfs/bin/hadoop namenode -format
5. ~/persistent-hdfs/bin/start-all.sh
6. ./persistent-hdfs/bin/hadoop dfs -mkdir /inputdir
Now change spark configuration to use persistent-hdfs instead of ephemeralhdfs and set default directory from /mnt to /vol0.

Go to spark config: /spark/conf/core-site.xml
1. Change the hdfs url port from 9000 to 9010
2. Change spark-env.sh
3. spark default directory from /mnt to /vol0
4. Restart Spark "Go to sbin and stop-all.sh then start-all.sh
For spark experiment I have created 7 slaves and 1 master node with data set of 128 GB and 1024 GB.

4.MPI BEnchmarking

Running mpi on single node
The setup for single node is easy. We have written the steps below 
tar -xzf mpich2-1.4.tar.gz
cd mpich2-1.4
./configure --disable-fortran
make; sudo make install

Compile your 3 files:

gcc -o TeraSort mpi_Terasort.c 
mpii -o sort sort.c
mpii -o merge merge.c

Now Run you file
./Terasort input_file_name

Running mpi on 8 node cluster
sudo easy_install StarCluster
starcluster help
Since StarCluster is not configured, it will print out the following 
StarCluster - (http://web.mit.edu/starcluster) (v. 0.93.3)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu

!!! ERROR - config file /Users/wesleykendall/.starcluster/config does not exist

Options:
--------
[1] Show the StarCluster config template
[2] Write config template to /Users/wesleykendall/.starcluster/config
[q] Quit

Please enter your selection:
Enter the number 2 and StarCluster will generate a default configuration file in your home directory under ~/.starcluster/config.
Obtain your AWS access key, secret access key, and your 12-digit user ID from your AWS account.
open your default config file (~/.starcluster/config) with your favorite text editor. Find the line with [aws info] and enter all of your AWS information into the proper fields:
AWS_ACCESS_KEY_ID = # Your Access Key ID here
AWS_SECRET_ACCESS_KEY = # Your Secret Access Key here
AWS_USER_ID = # Your 12-digit AWS Account ID here (no hyphens)

save the config file and then create a public/private key pair that will be uploaded to Amazon and used to authenticate your machine when you log into your cluster

starcluster createkey mykey -o ~/.ssh/mykey.rsa
configuring cluster parameters
KEYNAME = mykey
CLUSTER_SIZE = 8
CLUSTER_USER = abhinav@ 
CLUSTER_SHELL = bash
NODE_IMAGE_ID = virtual machine image
NODE_INSTANCE_TYPE = i3.large

enable the mpich2 plugin for Starcluster.Compile your program in the same way as single node and run your program.
                                      









	

